<p><img src="http://blog.canstack.ca/images/blog/cloud_photophilde.jpg" alt="" /><br />
<em>(<a href="http://www.flickr.com/photos/photophilde/2750643120/">photocredit</a>)</em></p>
<p class="meta">14 February &#8211; 2013 &#8211; Edmonton!</p>
<h1>Virtual computing lab OpenStack reference architecture</h1>
<p>One of the projects that I work on at <a href="http://cybera.ca">Cybera</a> is a <a href="http://www.cybera.ca/projects/virtual-computing-lab">virtual computing lab</a>. This project uses <a href="https://cwiki.apache.org/VCL/">Apache <span class="caps">VCL</span></a> backed by <a href="http://openstack.org">OpenStack</a> to provide users the ability to use, and reserve, virtual machines to use for their classes. Typically this means making a reservation through <span class="caps">VCL</span> and being able to login to a remote Windows instance to access software they otherwise might only be able to use in a physical computing lab.</p>
<p>Technically OpenStack isn&#8217;t 100% supported by Apache <span class="caps">VCL</span> right now, so we have some custom modifications in place that allow us to use OpenStack Essex as the virtual machine back-end, but I believe OpenStack support is on the road map.</p>
<h2>tl;dr</h2>
<p>For a reference, here is what we are using to provide OpenStack to our Apache <span class="caps">VCL</span> system.</p>
<p>- OpenStack version: <a href="http://www.openstack.org/software/essex/">Essex</a><br />
- OpenStack components used: nova-compute, nova-volume (ie. no swift)<br />
- OpenStack networking: <span class="caps">VLAN</span> Manager<br />
- Number of compute nodes: 7<br />
- Number of cloud controllers: 1<br />
- Servers: 2x <a href="http://www.dell.com/us/enterprise/p/poweredge-c6220/pd">Dell C6220</a> chassis with 4 nodes in each 2U chassis, for a total of 8 &#8220;servers.&#8221; Each node has 2x 10G <span class="caps">SFP</span> network ports, 2x 1G ports, and a 1G managment/<span class="caps">IPMI</span> port<br />
- Storage: Each node has, for now, 6x 1TB <span class="caps">SATA</span> drives. <br />
- 1x Arista 10G switch<br />
- 1x Cisco 1G management switch (for <span class="caps">IPMI</span>)<br />
- Operating system: Ubuntu 12.04 / Precise<br />
- Configuration management: <a href="http://ansible.cc">ansible</a><br />
- Baremetal deployment: <span class="caps">PXE</span> boot<br />
- Datacenter host: <a href="http://www.blackbridge.com/">Blackbridge</a></p>
<h2>Our architecture for the virtual computing lab</h2>
<p>As noted above, we are running OpenStack Essex. This is because it was the latest release available when we started the project. We are running OpenStack off of Ubuntu 12.04 and using the standard OpenStack packages. I haven&#8217;t run into any bugs yet, though because of an issue with our Windows images I set the <span class="caps">DHCP</span> timeout to 28800 instead of the default one minute, and we have configured OpenStack to use virtio on the bridges to get full use of our bonded 10G networking. Otherwise the virtual machines will be limited to 100M by default.</p>
<p>OpenStack is configured using the <a href="http://ansible.cc">Ansible</a> configuration managment system. Currently our Apache <span class="caps">VCL</span> instance is configured with <a href="http://wiki.opscode.com/display/chef/Home">Chef</a>, though it was previously configured via Ansible.</p>
<p>I like Ansible because it is straightforward, only requires a couple of python packages to run (paramiko and jinja2 templates), and uses ssh to access the servers. I wrote a <a href="http://www.cybera.ca/tech-radar/first-look-ansible">blog post</a> on Ansible a while back for Cybera so I won&#8217;t get into it too much here. Eventually I will probably be forced to use Chef or Puppet, but for now I&#8217;m quite happy with Ansible. :)</p>
<p>As far as networking, each node has 2x 10G <span class="caps">SFP</span> ports connected to a single 24 port <a href="http://www.aristanetworks.com/">Arista</a> switch. Also the <span class="caps">IPMI</span> interface is connected to an older, slower switch. The 10G nics are bonded in the OS and we have two virtual <span class="caps">VLAN</span> interfaces that have their raw device set as the bonded interface. In some of our other clouds we have two switches for high availability, but right now, for this project, we just have the one switch. Though with the two bonded connections we can easily add a new switch in the future.</p>
<p>If you haven&#8217;t heard of Arista before, now is a good time to look into what they are doing. Each switch runs on a <a href="http://en.wikipedia.org/wiki/Arista_Networks#Extensible_Operating_System">stock Linux kernel</a>, so you can run Linux applications on the switch if you want. Very interesting hardware and software. In the future I&#8217;ll try to share more about how the switches are actually configured.</p>
<p>Our datacenter host is <a href="http://www.blackbridge.com/">Blackbridge</a>. They are located near <a href="http://blackbridgenetworks.com/location.html">Lethbridge, Alberta, Canada</a>, which is about 600Km from my workplace on the <a href="http://ualberta.ca">UofA</a> Campus. I&#8217;ve never actually seen the servers! Blackbridge has been great to work with, even going so far as to take some pictures of our rack so that I could put them up on the CanStack website.</p>
<p><img src="http://blog.canstack.ca/images/blog/blackbridge_lethbridge.jpg" alt="" /></p>
<h2>Stateless virtual machines, over-committing and solid state drives</h2>
<p>As mentioned, right now the servers have 6x 1TB <span class="caps">SATA</span> drives. Our <span class="caps">VCL</span> project is interesting because the virtual machines are stateless. What I mean by that is if the virtual machine goes down for some reason&#8212;eg. hardware failure on a compute node&#8212;there will be no user data lost. Certainly if a user is accessing a virtual machine they will have to create a new reservation, but they don&#8217;t store their data on the actual instance, so they can&#8217;t lose data (assuming they save locally every once in a while)&#8212;all they have to do is make a new reservation and keep working.</p>
<p>The six <span class="caps">SATA</span> drives are configured in software-based RAID10. They are only really capable of about 400 <span class="caps">IOPS</span>. This is not very much, especially when working with <span class="caps">IOPS</span> hungry Windows instances. Each Windows instance uses about four or five <span class="caps">IOPS</span> even when &#8220;idle&#8221; so it doesn&#8217;t take very many instances to eat up all the available <span class="caps">IOPS</span>.</p>
<p>In the next couple of months we are going to roll out solid state drives to each of the compute nodes. Given that <span class="caps">VCL</span> is stateless, I have been doing some <a href="http://www.cybera.ca/tech-radar/performance-testing-solid-state-drives">testing</a> with striped/RAID0 configurations and when even the slowest of solid state drives can do 25k <span class="caps">IOPS</span> it doesn&#8217;t take many to achieve tens of thousands of <span class="caps">IOPS</span>.</p>
<p>In my experiments I&#8217;ve found we can benefit from at least 5k <span class="caps">IOPS</span> to support Windows bootstorms and such, so with 10x or 100x more <span class="caps">IOPS</span> we should see much improved Windows performance in <span class="caps">VCL</span>. Having more <span class="caps">IOPS</span> will also mean we can over-commit with <span class="caps">KVM</span>, because the reality is that over-committing is, in my opinion, mostly <span class="caps">IOPS</span>-bound. If we can over-commit we can run more virtual machines and provide access to more classes and students with a lower capital cost.</p>
<h2>Questions</h2>
<p>I can&#8217;t think of anything else important to mention, but if you have questions please feel free to ask them in the comments and I&#8217;ll be sure to respond.</p>
<p>Thanks,<br />
<a href="http://serverascode.com">Curtis</a></p>